{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT1lR7Snessy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Libraries\n",
        "import torch\n",
        "from torchtext import data\n",
        "import numpy as np\n",
        "import re\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import randint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPhc_T5DLWPo",
        "colab_type": "code",
        "outputId": "3037bcff-3e07-406c-8fa3-a862db43e66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# transfer the model on GPU\n",
        "# check GPU available\n",
        "if torch.cuda.is_available():\n",
        "    cuda0 = torch.device(\"cuda:0\") \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    cuda0 = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9Smc0dDctE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading custom NLP files using TabularDataset\n",
        "train_file = data.TabularDataset(\n",
        "    path ='/content/drive/My Drive/Colab Notebooks/data/wiki.train.tokens.txt', \n",
        "    format='tsv',\n",
        "    fields=[('text', data.Field())]\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dtOMYoIHNBG",
        "colab_type": "code",
        "outputId": "29c163b1-8a99-4116-b943-ae81223b9586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Total lines in train file : \",len(train_file))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total lines in train file :  36718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYIyFi9U2cK9",
        "colab_type": "text"
      },
      "source": [
        "The training data has : 1) Title, 2)Contents\n",
        "\n",
        "One example is shown below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5u2NYrzM47E",
        "colab_type": "code",
        "outputId": "828e16e8-0872-49e8-c794-e7eba3066304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "ex = train_file[1]\n",
        "print(\"Title : \",\" \".join(ex.text))\n",
        "ex = train_file[3]\n",
        "print(\"Content : \",\" \".join(ex.text))\n",
        "ex = train_file[2]\n",
        "print(\"train_file[2] :\", ex.text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title :  = Valkyria Chronicles III =\n",
            "Content :  Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" .\n",
            "train_file[2] : []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S94-d8u2mf7",
        "colab_type": "text"
      },
      "source": [
        "By looking at the data it is clear that there is \n",
        "\n",
        "*   Punctuations (e.g ':', '.', '@' etc)\n",
        "*   Strange names (e.g Valkyria)\n",
        "*   Quoted dialogue\n",
        "*   Empty data [ ] (e.g train_file[2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDyIW6x-5Tc4",
        "colab_type": "text"
      },
      "source": [
        "### Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF1cXUqU5goB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text_and_tokenization(rawfile):\n",
        "  # text cleaning\n",
        "  clean_doc=[]\n",
        "  for i in range (len(rawfile)):\n",
        "    if rawfile[i].text:\n",
        "      clean_doc.append(rawfile[i].text)\n",
        "\n",
        "  clean_doc = str(clean_doc).lower()\n",
        "  clean_doc = re.sub(r'[\\'|:|.|(|)|,|@|-|\"\\]|*|&|$|?|=|;]','',clean_doc)\n",
        "  clean_doc = re.sub(r'\\[','',clean_doc)    \n",
        "  clean_doc = re.sub(r'-',' ',clean_doc)    # removes -\n",
        "  clean_doc = re.sub(r'\\'','',clean_doc)    # removes '\n",
        "  clean_doc = re.sub(r'\\d+','',clean_doc)   # remove digits\n",
        "  clean_doc = re.sub(r' +',' ',clean_doc)   # removes more than one spaces\n",
        "  \n",
        "  # Tokenization\n",
        "  doc_tokenize=[]\n",
        "  for word in clean_doc.split(\" \"):\n",
        "    if word:\n",
        "      doc_tokenize.append(word)\n",
        "  return doc_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs-P_RzS34XH",
        "colab_type": "code",
        "outputId": "5fb96491-03f0-43d3-a5e8-84981ad35eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "clean_doc_tokenized = clean_text_and_tokenization(train_file)\n",
        "print(\"Total number of words : \", len(clean_doc_tokenized))\n",
        "print(\"unique number of words : \", len(set(clean_doc_tokenized)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words :  1701090\n",
            "unique number of words :  27401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4aIx-Zr_nmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence_length = 10+1 # 11 length sequence words, first 10 will be input words and 11th is the target/output\n",
        "def create_dataset(clean_doc_tokenized,N):\n",
        "  dataset =[]\n",
        "  for i in range(len(clean_doc_tokenized)-N+1):\n",
        "    context = clean_doc_tokenized[i:i+N-1]\n",
        "    target = clean_doc_tokenized[i+N-1]\n",
        "    dataset.append((context,target))\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23LgHjY145lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For the ease of training, i am considering only first 100K words from corpus\n",
        "clean_doc_tokenized_truncated = clean_doc_tokenized[:10000+1] \n",
        "dataset=create_dataset(clean_doc_tokenized_truncated,sequence_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z2BZ9H35Yto",
        "colab_type": "text"
      },
      "source": [
        "### Embedding + Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-QI_gtv5E1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2430deba-40e7-4199-caf1-18ba9d2d86bf"
      },
      "source": [
        "unique_words = set(clean_doc_tokenized_truncated) # unique words present in the corpus\n",
        "word_to_ids={}\n",
        "for i,word in enumerate(unique_words):\n",
        "  word_to_ids[word] = i\n",
        "\n",
        "print(\"vocabulary size :\", len(unique_words))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size : 2318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oLVxTMbJiC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "    super(NGramLanguageModeler, self).__init__()\n",
        "    # nn.Embedding uses \"encoding lexical sementics\" technique\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)     #embedding layer initialization\n",
        "    self.linear1 = nn.Linear(context_size * embedding_dim, 4096)  #linear layer\n",
        "    self.linear2 = nn.Linear(4096, vocab_size)                    #output layer , out dim = vocab_size\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    embeds = self.embeddings(inputs).view((1, -1))\n",
        "    out = F.relu(self.linear1(embeds))\n",
        "    out = self.linear2(out)\n",
        "    log_probs = F.log_softmax(out, dim=1)\n",
        "    return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug7Hvx-4KvMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model):\n",
        "  model = model.to(cuda0)  # to transfer model on gpu\n",
        "  losses=[]\n",
        "  i=1\n",
        "  for epoch in range(no_of_epoches):\n",
        "    loss_per_epoch =0.0\n",
        "    for context, target in dataset:\n",
        "      context_ids = torch.tensor([word_to_ids[w] for w in context] ,dtype=torch.long )\n",
        "      context_ids = context_ids.to(cuda0) # transfer input on gpu\n",
        "      target_ids = torch.tensor([word_to_ids[target]], dtype=torch.long)\n",
        "      target_ids = target_ids.to(cuda0) # transfer output on gpu\n",
        "      model.zero_grad()\n",
        "      output = model(context_ids)\n",
        "      loss = loss_function(output,target_ids)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_per_epoch += loss.item()\n",
        "\n",
        "    print(\"EPOCH : \"+str(epoch+1)+\" | Loss : \"+str(loss_per_epoch/9991))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCjdLnBfQWT-",
        "colab_type": "code",
        "outputId": "b8bff43c-b571-4743-a08c-583c2b406816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#Hyperparameters\n",
        "embedding_dimensions = 10 # one word is converted into a vector of 10 dimensions\n",
        "context_size = sequence_length -1       # no of input words for training the model\n",
        "learning_rate = 0.001     \n",
        "momentum = 0.9\n",
        "no_of_epoches =7\n",
        "\n",
        "#defining model\n",
        "model = NGramLanguageModeler(len(unique_words), embedding_dimensions, context_size)\n",
        "# defining loss function as negative log likelihood loss\n",
        "loss_function = nn.NLLLoss()\n",
        "# defining optimizer as stochastic gradient descent\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "#Train the model\n",
        "train_model(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH : 1 | Loss : 6.6577825699467255\n",
            "EPOCH : 2 | Loss : 4.461004046727535\n",
            "EPOCH : 3 | Loss : 2.59800486936427\n",
            "EPOCH : 4 | Loss : 0.9943666979305117\n",
            "EPOCH : 5 | Loss : 0.3737648253373562\n",
            "EPOCH : 6 | Loss : 0.22689246281812933\n",
            "EPOCH : 7 | Loss : 0.14445581031435067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6g0vWXdDDE0",
        "colab_type": "text"
      },
      "source": [
        "### Generating text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iP2vDjGFn4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_words(N_words,model,input_text):\n",
        "  print(\"Input sequence of given words : \",\" \".join(input_text))\n",
        "  text = \"\"\n",
        "  # put model on evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  for i in range(N_words):\n",
        "    input_text_ids = torch.tensor([word_to_ids[w] for w in input_text], dtype=torch.long)\n",
        "    input_text_ids = input_text_ids.to(cuda0)\n",
        "    output = model(input_text_ids)\n",
        "    _, max_index  = torch.max(output, 1) # picking the maximum probability word\n",
        "    for word, index in word_to_ids.items():\n",
        "      if index == max_index.item():\n",
        "        text = text+' '+word\n",
        "        input_text.append(word)\n",
        "        break\n",
        "    input_text = input_text[1:]\n",
        "\n",
        "  print(\"Generated words :\", text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njFzeqkXM6BM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e7b11bc8-bf61-41bd-fcf8-9593742fa467"
      },
      "source": [
        "input_sequence_of_words = dataset[randint(0, len(dataset))][0]\n",
        "N_words = 20 # number of words to be generated\n",
        "generate_words(N_words,model,input_sequence_of_words)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence of given words :  take possession of the arsenal in the name of the\n",
            "Generated words :  united states the soldiers would be allowed safe passage in any direction carrying any personal and public property besides munitions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vae2PcscuVjW",
        "colab_type": "text"
      },
      "source": [
        "**Note:** The training corpus has 100K words, and so the vocabulary size is around 2K. With the increase in training corpus and vocab size, the generated words will be more diversified.\n",
        "\n",
        "Moreover, no pre-trained embedding is used in this code. nn.Embedding layer from pytorch is used to create word vectors implicitly."
      ]
    }
  ]
}